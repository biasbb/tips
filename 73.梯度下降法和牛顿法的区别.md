- 梯度下降法是一阶优化算法，牛顿法是二阶优化算法
- 牛顿法的收敛速度相比梯度下降法常常较快
- 牛顿法每次需要更新一个二维矩阵，计算代价很大，实际使用中常使用拟牛顿法
- **牛顿法对初始值有一定要求，在非凸优化问题中（如神经网络训练），牛顿法很容易陷入鞍点（牛顿法步长会越来越小），而梯度下降法则很容易逃离鞍点（因此在神经网络训练中一般使用梯度下降法，高维空间的神经网络中存在大量鞍点）**
- 梯度下降法在靠近最优点时会震荡，因此步长调整在梯度下降法中是必要的，具体有adagrad, adadelta, rmsprop, adam等一系列自适应学习率的方法
  

